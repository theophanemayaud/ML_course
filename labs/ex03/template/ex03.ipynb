{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Least squares and linear basis functions models\n",
    "## 1.1 Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from th.th_costs import compute_loss\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\" calculate the least squares solution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : the vector of answer values \n",
    "    tx : the matrix of input values with augmented \n",
    "        first column of 1 to account for constant w0 parameter. \n",
    "        Rows are datapoints of D dimensions, \n",
    "        and columns are features of N dimensions\n",
    "    out : mse, w\n",
    "    \"\"\"\n",
    "                    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # least squares: TODO\n",
    "    \n",
    "    # There are two methods (both tested and working)\n",
    "    \n",
    "    # 1. Manual method with inverse\n",
    "    # w = np.dot(np.dot(np.linalg.inv(np.dot(tx.T, tx)), tx.T), y)\n",
    "\n",
    "    # 2. With np solving method, solves x in Ax=b, here we have XˆT*X*w_star = XˆT*y => More robust method because works even if not inversible.\n",
    "    w = np.linalg.solve(np.dot(tx.T, tx), np.dot(tx.T, y))\n",
    "    \n",
    "    mse = compute_loss(y, tx, w)\n",
    "    return mse, w\n",
    "    \n",
    "    # returns mse, and optimal weights\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Here we will reuse the dataset `height_weight_genders.csv` from previous exercise section to check the correctness of your implementation. Please compare it with your previous result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from th.th_grid_search import generate_w, get_best_parameters, grid_search\n",
    "from th.th_gradient_descent import gradient_descent\n",
    "\n",
    "from helpers import *\n",
    "def test_your_least_squares():\n",
    "    height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n",
    "    x, mean_x, std_x = standardize(height)\n",
    "    y, tx = build_model_data(x, weight)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # least square or grid search: TODO\n",
    "    \n",
    "    # First grid search\n",
    "    # Generate the grid of parameters to be swept\n",
    "    grid_w0, grid_w1 = generate_w(num_intervals=200) #NB this function creates num_intervals points regularily spaced between -100 and 200 for w0 and -150 and 150 for w1\n",
    "\n",
    "    # Start the grid search\n",
    "    grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "    # Select the best combinaison\n",
    "    loss_star_gs, w0_star_gs, w1_star_gs = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "    print(f\"Grid search best loss={loss_star_gs}, w_star params are=[{w0_star_gs}   {w1_star_gs}]\\n\")\n",
    "    \n",
    "    # Now least squares method\n",
    "    mse_ls, w_star_ls = least_squares(y, tx)\n",
    "    print(f\"Least squares best loss={mse_ls}, w_star params={w_star_ls}\\n\")\n",
    "    \n",
    "    # Now gradient descent method\n",
    "    mse_gd, w_star_gd = gradient_descent(y, tx, initial_w=[0, 0], max_iters=100, gamma=0.7)\n",
    "    print(f\"Gradient descent best loss={mse_gd[len(mse_gd)-1]}, w_star params={w_star_gd[len(w_star_gd)-1]}\\n\")\n",
    "\n",
    "    # this code should compare the optimal weights obtained \n",
    "    # by least squares vs. grid search\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_your_least_squares()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Least squares with a linear basis function model\n",
    "Start from this section, we will use the dataset `dataEx3.csv`.\n",
    "\n",
    "### Implement polynomial basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "x, y = load_data()\n",
    "print(\"shape of x {}\".format(x.shape))\n",
    "print(\"shape of y {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    phi_x : matrix formed of augmented features, \n",
    "        from x = [x1, x2, x3...].T it returns\n",
    "        phi_x = [[1, x1, x1ˆ2, x1ˆ3, ..., x1ˆdegree],\n",
    "                 [1, x2, x2ˆ2, x2ˆ3, ..., x2^degree],\n",
    "                 ...]\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # polynomial basis function: TODO\n",
    "    # this function should return the matrix formed\n",
    "    # by applying the polynomial basis to the input data\n",
    "    \n",
    "    phi_x = np.empty(shape=(len(x), degree+1))\n",
    "    for feature_i in range(len(x)):\n",
    "        for degree_i in range(degree+1):\n",
    "            phi_x[feature_i][degree_i] = np.power(x[feature_i], degree_i)\n",
    "            \n",
    "    return phi_x\n",
    "    \n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "   \n",
    "# THEO my test code\n",
    "build_poly(np.array([2,1, 3]).T, degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with polynomial regression. Note that we will use your implemented function `compute_mse`. Please copy and paste your implementation from exercise02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import *\n",
    "\n",
    "def polynomial_regression():\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 3, 7, 12]\n",
    "    \n",
    "    # define the structure of the figure\n",
    "    num_row = 2\n",
    "    num_col = 2\n",
    "    f, axs = plt.subplots(num_row, num_col)\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # form the data to do polynomial regression.: TODO\n",
    "        \n",
    "        tx = build_poly(x, degree)\n",
    "        \n",
    "        # ***************************************************\n",
    "        #raise NotImplementedError\n",
    "        \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # least square and calculate RMSE: TODO\n",
    "        \n",
    "        mse_ls, weights = least_squares(y, tx)\n",
    "        rmse = np.sqrt(2*mse_ls)\n",
    "        \n",
    "        # ***************************************************\n",
    "        #raise NotImplementedError\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse))\n",
    "        # plot fit\n",
    "        plot_fitted_curve(\n",
    "            y, x, weights, degree, axs[ind // num_col][ind % num_col])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"visualize_polynomial_regression\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Evaluating model predication performance\n",
    "\n",
    "\n",
    "Let us show the train and test splits for various polynomial degrees. First of all, please fill in the function `split_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    \n",
    "    if ratio<0 or ratio>1:\n",
    "        raise NameError(\"Ratio is out of [0 1] range\")\n",
    "\n",
    "    nb_data_pts = len(y)\n",
    "    nb_train_pts = int(np.rint(nb_data_pts*ratio))\n",
    "    \n",
    "    train_true_false = np.full(nb_data_pts, False)\n",
    "\n",
    "    train_pts_indexes = np.random.choice(np.arange(start=0, stop=nb_data_pts), size=nb_train_pts, replace=False) # high is actually one above the max possible integer the function might return\n",
    "    train_true_false[train_pts_indexes] = True\n",
    "    \n",
    "    train_x = x[train_true_false]\n",
    "    train_y = y[train_true_false]\n",
    "    \n",
    "    test_x = x[~train_true_false]\n",
    "    test_y = y[~train_true_false]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "    \n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "# THEO test my code : \n",
    "split_data(x=np.array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [100, 101, 102, 103, 104, 105, 106, 107, 108, 109]]).T, y=np.array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]).T, ratio=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, test your `split_data` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from th.th_costs import compute_rmse\n",
    "from th.th_split_data import split_data\n",
    "\n",
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data, and return train and test data: TODO\n",
    "    \n",
    "    train_x, train_y, test_x, test_y = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    \n",
    "    train_x_aug = build_poly(train_x, degree)\n",
    "    test_x_aug = build_poly(test_x, degree)\n",
    "    \n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate weight through least square: TODO\n",
    "    \n",
    "    train_mse, train_w_ls = least_squares(train_y, train_x_aug)\n",
    "    \n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate RMSE for train and test data,\n",
    "    # and store them in rmse_tr and rmse_te respectively: TODO\n",
    "        \n",
    "    rmse_tr = np.sqrt(2*train_mse)\n",
    "    rmse_te = compute_rmse(test_y, test_x_aug, train_w_ls)\n",
    "\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.5, 0.1]\n",
    "\n",
    "print(\"Sorted by proportion\")\n",
    "for split_ratio in split_ratios:\n",
    "    for degree in degrees:\n",
    "        train_test_split_demo(x, y, degree, split_ratio, seed)\n",
    "    print('\\n')\n",
    "    \n",
    "print(\"Sorted by degree\")\n",
    "for degree in degrees:\n",
    "    for split_ratio in split_ratios:\n",
    "        train_test_split_demo(x, y, degree, split_ratio, seed)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Ridge Regression\n",
    "Please fill in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in : y, tx, lambda_\n",
    "    out : w\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    w : computed wheights with ridge regression\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    \n",
    "    \n",
    "    #Very similar code to least_sqares\n",
    "    \n",
    "    #There are two methods for solving the algebric system (both tested and working)\n",
    "    # We want to solve the system Aw_star = b with b = X_T*y\n",
    "    \n",
    "    A = np.dot(tx.T, tx) + 2*len(y)*lambda_*np.identity(len(tx.T))\n",
    "    b = np.dot(tx.T, y)\n",
    "    \n",
    "    # 1. Manual method with inverse\n",
    "    # w = np.dot(np.linalg.inv(A), b)\n",
    "\n",
    "    # 2. With np solving method, solves x in Ax=b, here we have XˆT*X*w_star = XˆT*y => More robust method because works even if not inversible.\n",
    "    w = np.linalg.solve(A, b)\n",
    "    \n",
    "    return w\n",
    "    \n",
    "    \n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "# THEOTEST code\n",
    "test_w = ridge_regression(\n",
    "    y = np.array([[10, 11, 12, 13, 16, 18, 19]]).T,\n",
    "    tx = np.array([[  1.,   0.,   0.,   0.],\n",
    "                  [  1.,   1.,   1.,   1.],\n",
    "                  [  1.,   2.,   4.,   8.],\n",
    "                  [  1.,   3.,   9.,  27.],\n",
    "                  [  1.,   6.,  36., 216.],\n",
    "                  [  1.,   8.,  64., 512.],\n",
    "                  [  1.,   9.,  81., 729.]]),  \n",
    "    lambda_ = 0\n",
    ")\n",
    "print(\"Ridge w =\\n\", test_w, '\\n')\n",
    "\n",
    "_, test_w = least_squares(\n",
    "    y = np.array([[10, 11, 12, 13, 16, 18, 19]]).T,\n",
    "    tx = np.array([[  1.,   0.,   0.,   0.],\n",
    "                  [  1.,   1.,   1.,   1.],\n",
    "                  [  1.,   2.,   4.,   8.],\n",
    "                  [  1.,   3.,   9.,  27.],\n",
    "                  [  1.,   6.,  36., 216.],\n",
    "                  [  1.,   8.,  64., 512.],\n",
    "                  [  1.,   9.,  81., 729.]]))\n",
    "print(\"Least squares w=\\n\", test_w, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE    \n",
    "    # split the data, and return train and test data: TODO\n",
    "    \n",
    "    train_x, train_y, test_x, test_y = split_data(x, y, ratio, seed)\n",
    "\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    \n",
    "    train_x_aug = build_poly(train_x, degree)\n",
    "    test_x_aug = build_poly(test_x, degree)\n",
    "    \n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE        \n",
    "        # ridge regression with a given lambda\n",
    "        \n",
    "        w = ridge_regression(train_y, train_x_aug, lambda_)\n",
    "        \n",
    "        rmse_tr.append(compute_rmse(train_y, train_x_aug, w))\n",
    "        rmse_te.append(compute_rmse(test_y, test_x_aug, w))\n",
    "        \n",
    "        # ***************************************************\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "\n",
    "    #raise NotImplementedError\n",
    "\n",
    "# THEOTEST code\n",
    "# ridge_regression_demo(x=np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).T, \n",
    "#                       y=np.array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]).T,\n",
    "#                       degree = 3,\n",
    "#                       ratio = 0.7,\n",
    "#                       seed = 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
