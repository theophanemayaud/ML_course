{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Computing the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `compute_loss` function below:\n",
    "<a id='compute_loss'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    #print(\"Y shape\" , y.shape, \"\\n\\n\")\n",
    "    #print(\"w shape\" , w.shape, \"\\n\\n\")\n",
    "    #print(\"TX shape\" , tx.shape, \"\\n\\n\")\n",
    "    mse = np.dot( (y-np.dot(tx, w)).T, y-np.dot(tx, w))/ (2*y.shape[0])\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    return mse # THEOADDED\n",
    "\n",
    "#y = np.array([4,7,6,5]).T\n",
    "#w = np.array([3,1]).T\n",
    "#tx = np.array([[1,1,1,1], [1,2,3,4]]).T\n",
    "\n",
    "w = np.array([1,2]).T\n",
    "#print(\"w shape\" , w.shape, \"\\n\\n\")\n",
    "print(compute_loss(y, tx, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Grid Search\n",
    "\n",
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    losses = -1*np.ones((len(w0), len(w1))) #set to -1 as losses can only be positive\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    w0_star, w1_star, loss_star = -1, -1, -1\n",
    "    for w0_i in range(w0.shape[0]):\n",
    "        for w1_i in range(w1.shape[0]):\n",
    "            losses[w0_i][w1_i] = compute_loss(y, tx, np.array([w0[w0_i],w1[w1_i]]).T)\n",
    "            if loss_star == -1 or loss_star > losses[w0_i][w1_i] :\n",
    "                loss_star = losses[w0_i][w1_i]\n",
    "                w0_star = w0[w0_i]\n",
    "                w1_star = w1[w1_i]\n",
    "    #print(loss_star, w0_star, w1_star)\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    return losses\n",
    "\n",
    "#grid_search(y, tx, np.array([3,4,5]).T, np.array([1,7,2]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=100) #NB this function createts num_intervals points regularily spaced between -100 and 200 for w0 and -150 and 150 for w1\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    N = y.shape[0] # N is the number of datapoints\n",
    "    # tx is the x input matrix with the augmented 1 column at the beginning for the w0 parameter as the offset at axis origins\n",
    "    e = y - np.dot(tx, w)# e is the error vector e = y - f(x). NB there is a calculated error for each datapoint\n",
    "    gradient = -np.dot(tx.T, e)/ N\n",
    "    return gradient\n",
    "    # TODO: compute gradient and error vector\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "   \n",
    "\n",
    "# y = np.array([[4],\n",
    "#               [7],\n",
    "#               [6],\n",
    "#               [5],\n",
    "#               [10]])\n",
    "\n",
    "# w = np.array([[0.4],\n",
    "#               [1.],\n",
    "#               [0.6]])\n",
    "\n",
    "# tx = np.array([[1,1,5],\n",
    "#                [1,2,7],\n",
    "#                [1,3,9],\n",
    "#                [1,4,1],\n",
    "#                [1,2,9]])\n",
    "\n",
    "# compute_gradient(y, tx, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        w = ws[n_iter]\n",
    "        #print(w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        #print(loss)\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        #raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        w = w - gamma*gradient\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        #raise NotImplementedError\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws\n",
    "\n",
    "\n",
    "# y = np.array([[4],\n",
    "#               [5],\n",
    "#               [6],\n",
    "#               [7]])\n",
    "\n",
    "# w = np.array([[0.4],\n",
    "#               [1.]])\n",
    "\n",
    "# tx = np.array([[1,1],\n",
    "#                [1,2],\n",
    "#                [1,3],\n",
    "#                [1,4]])\n",
    "\n",
    "# gradient_descent(y, tx, w, 1000, 0.1)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    #print(loss)\n",
    "    gradient = compute_gradient(y, tx, w)    \n",
    "    w = w - gamma*gradient\n",
    "    return w\n",
    "    # TODO: implement stochastic gradient computation.It's same as the gradient descent.\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=batch_size, num_batches=max_iters):\n",
    "\n",
    "        loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "        gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "        w = w - gamma*gradient\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent: loss={l}, w0={w0}, w1={w1}\".format(l=loss, w0=w[0], w1=w[1]))\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Effect of Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "sgd_losses, sgd_ws = gradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving\n",
    "#plt.show()\n",
    "\n",
    "print(\"\\n\\n New test wih ouliers\\n\")\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "sgd_losses, sgd_ws = gradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Subgradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function `compute_loss(y, tx, w)` for the Mean Absolute Error cost function [here](#compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
